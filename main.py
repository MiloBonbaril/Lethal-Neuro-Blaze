import numpy as np
import cv2
import time
import torch
import os

# Importation de vos organes
from senses import TemporalRetina, BioMonitor, get_game_window
from brain import MotorCortex, ACTION_MAP
from agent import Agent

# --- HYPERPARAM√àTRES DE L'EXP√âRIENCE ---
WINDOW_TITLE = "LLBlaze"
EPISODES = 500              # Nombre de parties √† jouer
MAX_STEPS_PER_EPISODE = 2000 # S√©curit√© pour √©viter les boucles infinies
SAVE_INTERVAL = 10          # Sauvegarder le cerveau tous les X √©pisodes
MODEL_FILE = "neuro_blaze_v1.pth"

# R√©compenses (La chimie du plaisir et de la douleur)
REWARD_SURVIVAL = 0.1       # Joie d'√™tre en vie √† chaque frame
REWARD_DAMAGE = -50.0       # Douleur intense quand la vie baisse
REWARD_DEATH = -100.0       # Traumatisme final
REWARD_WIN = 100.0          # Extase de la victoire

def transmute_state(numpy_state):
    """
    Transforme la perception brute (Numpy HWC) en influx nerveux (Torch CHW).
    Entr√©e : (84, 84, 4) -> Sortie : (4, 84, 84)
    """
    # Transpose les axes : (2, 0, 1) met le canal (index 2) en premier
    return numpy_state.transpose(2, 0, 1)

def main():
    print("üß¨ INITIALISATION DU PROJET LETHAL NEURO-BLAZE...")

    # 1. Connexion aux Organes Sensoriels
    game_geo = get_game_window(WINDOW_TITLE)
    if not game_geo:
        print("‚ùå ERREUR CRITIQUE : Jeu introuvable. Lancez Lethal League Blaze.")
        return

    eye = TemporalRetina(game_geo)
    amygdala = BioMonitor(game_geo)
    muscles = MotorCortex()
    
    # 2. Naissance de l'Agent
    # Input shape pour l'agent : (Channels, Height, Width)
    input_shape = (4, 84, 84) 
    num_actions = len(ACTION_MAP) # Devrait √™tre 7 (0 √† 6)
    
    neuro_agent = Agent(input_shape, num_actions)

    # Chargement d'un cerveau existant si disponible (Transmigration)
    if os.path.exists(MODEL_FILE):
        print(f"üìÇ Cerveau existant d√©tect√©. Chargement des poids synaptiques...")
        neuro_agent.load(MODEL_FILE)
    else:
        print(f"üë∂ Cr√©ation d'un nouveau cerveau vierge.")

    print(f"\nüß† D√âBUT DE L'ENTRA√éNEMENT ({EPISODES} g√©n√©rations pr√©vues)")
    print("Passez sur la fen√™tre du jeu. L'IA prend le contr√¥le dans 5 secondes...")
    time.sleep(5)

    # --- BOUCLE DES √âPISODES (G√©n√©rations) ---
    for episode in range(1, EPISODES + 1):
        # Reset de l'√©tat pour une nouvelle partie
        # On vide un peu le buffer visuel pour ne pas voir la partie d'avant
        # Note: Id√©alement, on devrait avoir une fonction reset() dans la r√©tine
        print(f"--- √âpisode {episode} ---")
        
        # On capture l'√©tat initial
        current_state = transmute_state(eye.get_state())
        
        last_hp = 1.0 # On commence full life (ou on l'esp√®re)
        total_reward = 0
        step = 0
        
        while step < MAX_STEPS_PER_EPISODE:
            step += 1
            
            # A. D√âCISION (Le Cerveau choisit)
            action_idx = neuro_agent.select_action(current_state)
            
            # B. ACTION (Le Corps ex√©cute)
            muscles.execute(action_idx)
            
            # C. D√âLAI DE R√âACTION & OBSERVATION
            # On laisse un tout petit temps au jeu pour r√©agir (physique)
            # Si le jeu tourne √† 60FPS, 1 frame = ~0.016s.
            # On ne veut pas spammer trop vite.
            # time.sleep(0.01) # Optionnel, d√©pend de la vitesse de votre machine
            
            next_state_raw = eye.get_state()
            next_state = transmute_state(next_state_raw)
            
            # D. PERCEPTION DE LA R√âCOMPENSE (Amygdale)
            current_hp, _ = amygdala.read_hp()
            reward = 0
            done = False
            
            # Logique de Survie (Heuristique)
            # 1. Calcul de la diff√©rence de vie
            hp_delta = current_hp - last_hp
            
            if hp_delta < -0.01: # Perte de vie significative (Filtrage du bruit)
                # Si on passe brutalement √† 0 alors qu'on avait de la vie -> MORT
                if current_hp == 0 and last_hp > 0.1:
                    reward = REWARD_DEATH
                    done = True
                    print("üíÄ MORT DETECT√âE.")
                else:
                    # D√©g√¢ts standard
                    reward = REWARD_DAMAGE * abs(hp_delta) # Plus on a mal, plus c'est punitif
                    # print(f"ü©∏ D√©g√¢ts re√ßus ! Reward: {reward:.2f}")
            
            elif current_hp == 0 and last_hp < 0.1:
                # On √©tait d√©j√† mort ou presque, et on reste √† 0
                # C'est la fin de l'√©pisode (ou l'attente du respawn)
                done = True
            
            elif current_hp == 0 and last_hp > 0.1:
                 # Cas √©trange : HUD disparait alors qu'on allait bien -> VICTOIRE ?
                 # Dans le doute, on consid√®re cela comme une fin d'√©pisode positive
                 reward = REWARD_WIN
                 done = True
                 print("üèÜ VICTOIRE PROBABLE (Disparition HUD).")
            
            else:
                # On est en vie et stable
                reward = REWARD_SURVIVAL

            # Mise √† jour de la m√©moire imm√©diate
            last_hp = current_hp
            total_reward += reward

            # E. M√âMORISATION (Replay Buffer)
            # On stocke l'exp√©rience dans l'hippocampe
            neuro_agent.memory.push(current_state, action_idx, reward, next_state, done)

            # F. APPRENTISSAGE (Plasticit√© Synaptique)
            # L'agent r√™ve et optimise ses poids
            loss = neuro_agent.learn()
            
            # Transition d'√©tat
            current_state = next_state
            
            # Affichage p√©riodique (monitoring)
            if step % 100 == 0:
                print(f"Step {step} | Epsilon: {neuro_agent.epsilon:.3f} | HP: {current_hp:.2f}")

            if done:
                break
        
        # Fin de l'√©pisode
        neuro_agent.update_target_network()
        print(f"Fin √âpisode {episode}. Reward Total: {total_reward:.2f}. Steps: {step}")
        
        if episode % SAVE_INTERVAL == 0:
            neuro_agent.save(MODEL_FILE)
            print("üíæ Cerveau sauvegard√©.")
            
        # Pause pour laisser le jeu recharger (Menu, Replay...)
        # Vous devrez peut-√™tre appuyer manuellement sur 'A' pour relancer une partie
        # ou coder une fonction "press_continue" aveugle.
        time.sleep(3) 

if __name__ == "__main__":
    main()