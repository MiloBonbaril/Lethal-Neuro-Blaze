import time
import os
import config
from agent import Agent
from environment import Environment

import argparse
import sys

def run_training(env, neuro_agent, start_episode, best_reward):
    print(f"\nðŸ§  DÃ‰BUT DE L'ENTRAÃŽNEMENT (Ã‰pisode {start_episode}/{config.EPISODES})")
    print(">>> PLACEZ LE JEU EN PREMIER PLAN <<<")
    time.sleep(5)

    for episode in range(start_episode, config.EPISODES + 1):
        # 1. RESET (Attente rÃ©surrection + Initialisation)
        current_state = env.reset()
        
        total_reward = 0
        step = 0
        done = False
        
        print(f"--- Ã‰pisode {episode} ---")
        
        while step < config.MAX_STEPS_PER_EPISODE and not done:
            step += 1
            
            # A. DÃ‰CISION
            action_idx = neuro_agent.select_action(current_state)
            
            # B. ACTION & OBSERVATION (via Environment)
            next_state, reward, done, info = env.step(action_idx)
            
            total_reward += reward

            # C. MÃ‰MORISATION & APPRENTISSAGE
            neuro_agent.memory.push(current_state, action_idx, reward, next_state, done)
            loss = neuro_agent.learn()
            
            current_state = next_state
            
            # Monitoring lÃ©ger
            if step % 50 == 0:
                print(f"Step {step} (x{config.FRAME_SKIP}) | Eps: {neuro_agent.epsilon:.2f} | HP: {info['hp']:.2f} | R: {reward:.1f}")

        # Fin de l'Ã©pisode
        neuro_agent.update_target_network()
        print(f"ðŸ’€ Fin Ã‰pisode {episode}. Score: {total_reward:.2f}")

        if total_reward > best_reward:
            best_reward = total_reward
        
        if episode % config.SAVE_INTERVAL == 0:
            neuro_agent.save(config.MODEL_FILE, episode, best_reward)
            print("ðŸ’¾ Sauvegarde synaptique.")

def run_inference(env, neuro_agent):
    print(f"\nðŸ§  MODE EXÃ‰CUTION (INFERENCE ONLY)")
    print(">>> PLACEZ LE JEU EN PREMIER PLAN <<<")
    
    # Force epsilon Ã  0 pour dÃ©sactiver l'exploration (pure exploitation)
    neuro_agent.epsilon = 0.0
    print(f"Exploration (Epsilon) : {neuro_agent.epsilon}")
    
    # Mettre le modÃ¨le en mode Ã©valuation (dÃ©sactive dropout, batchnorm, etc. si utilisÃ©s)
    neuro_agent.policy_net.eval()
    
    time.sleep(5)

    episode = 1
    while True:
        # 1. RESET
        current_state = env.reset()
        
        total_reward = 0
        step = 0
        done = False
        
        print(f"--- Ã‰pisode {episode} (ExÃ©cution) ---")
        
        while step < config.MAX_STEPS_PER_EPISODE and not done:
            step += 1
            
            # A. DÃ‰CISION (Sans gradient)
            with torch.no_grad():
                # select_action utilise dÃ©jÃ  epsilon, et on l'a mis Ã  0.
                # Mais on peut appeler explicitement policy_net pour Ãªtre sÃ»r ou laisser select_action 
                # qui gÃ¨re dÃ©jÃ  le torch.no_grad() dans la branche exploitation.
                # On va utiliser select_action car il gÃ¨re le formatage de l'Ã©tat.
                action_idx = neuro_agent.select_action(current_state)
            
            # B. ACTION & OBSERVATION
            next_state, reward, done, info = env.step(action_idx)
            
            total_reward += reward
            current_state = next_state
            
            # Monitoring lÃ©ger
            if step % 50 == 0:
                 print(f"Step {step} | HP: {info['hp']:.2f} | R: {reward:.1f} | Action: {config.ACTION_MAP[action_idx]}")

        print(f"ðŸ’€ Fin Ã‰pisode {episode}. Score: {total_reward:.2f}")
        episode += 1
        time.sleep(2) # Petite pause entre les Ã©pisodes

def main():
    parser = argparse.ArgumentParser(description='Lethal Neuro-Blaze Agent')
    parser.add_argument('--mode', choices=['train', 'run'], default='train', help='Mode de fonctionnement: train (entraÃ®nement) ou run (exÃ©cution/infÃ©rence)')
    args = parser.parse_args()

    print("ðŸ§¬ INITIALISATION DU PROJET LETHAL NEURO-BLAZE (OPTIMISÃ‰)...")

    try:
        env = Environment()
    except Exception as e:
        print(f"Erreur d'initialisation de l'environnement: {e}")
        return

    input_shape = config.INPUT_SHAPE
    num_actions = len(config.ACTION_MAP)
    neuro_agent = Agent(input_shape, num_actions)

    start_episode = 1
    best_reward = -float('inf')

    if os.path.exists(config.MODEL_FILE):
        print(f"ðŸ“‚ Chargement du cerveau : {config.MODEL_FILE}")
        start_episode, best_reward = neuro_agent.load(config.MODEL_FILE)
    else:
        if args.mode == 'run':
            print("âš ï¸ AUCUN MODÃˆLE TROUVÃ‰ ! Impossible de lancer en mode exÃ©cution.")
            return

    if args.mode == 'train':
        run_training(env, neuro_agent, start_episode, best_reward)
    elif args.mode == 'run':
        run_inference(env, neuro_agent)

if __name__ == "__main__":
    main()